{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/foursquare_ai/blob/master/notebooks/15.railways.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the link to extract more information on railway stations:\n",
        "\n",
        "https://www.railyatri.in/stations?name=L&page=10"
      ],
      "metadata": {
        "id": "vKcxLNqpomag"
      },
      "id": "vKcxLNqpomag"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6795e6",
      "metadata": {
        "id": "0a6795e6"
      },
      "outputs": [],
      "source": [
        "import duckdb\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Optional\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract The Railway Station Locations"
      ],
      "metadata": {
        "id": "VTITdPQCrJzQ"
      },
      "id": "VTITdPQCrJzQ"
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "wget https://download.geofabrik.de/asia/india-latest.osm.pbf -O india-latest.osm.pbf"
      ],
      "metadata": {
        "id": "Oz5DRDjgWTud"
      },
      "id": "Oz5DRDjgWTud",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f278f1ef",
      "metadata": {
        "id": "f278f1ef"
      },
      "outputs": [],
      "source": [
        "con = duckdb.connect()\n",
        "\n",
        "con.execute(\"INSTALL httpfs;\")\n",
        "con.execute(\"LOAD httpfs;\")\n",
        "con.execute(\"INSTALL spatial;\")\n",
        "con.execute(\"LOAD spatial;\")\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    id,\n",
        "    unnest(map_extract(tags, 'name'))              AS name,\n",
        "    unnest(map_extract(tags, 'railway'))           AS railway,\n",
        "    unnest(map_extract(tags, 'public_transport'))  AS public_transport,\n",
        "    unnest(map_extract(tags, 'ref')) AS station_code,\n",
        "    unnest(map_extract(tags, 'network')) AS network,\n",
        "    unnest(map_extract(tags, 'internet_access')) AS internet_access,\n",
        "    lat,\n",
        "    lon\n",
        "FROM ST_READOSM('india-latest.osm.pbf')\n",
        "WHERE kind = 'node'\n",
        "AND 'station' IN map_extract(tags, 'railway')\n",
        "AND 'station' IN map_extract(tags, 'public_transport')\n",
        "\"\"\"\n",
        "\n",
        "df = con.execute(query).df()\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ClRFqqfeXJKm"
      },
      "id": "ClRFqqfeXJKm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "diV9Hh9raC75"
      },
      "id": "diV9Hh9raC75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract the more info on Railway Stations"
      ],
      "metadata": {
        "id": "SzOMy0enrPiE"
      },
      "id": "SzOMy0enrPiE"
    },
    {
      "cell_type": "code",
      "source": [
        "letters = list(string.ascii_uppercase)\n",
        "\n",
        "\n",
        "BASE_URL = \"https://www.railyatri.in/stations\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "        \"(KHTML, like Gecko) Chrome/120.0 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "\n",
        "def _get_soup(letter: str, page: int) -> BeautifulSoup:\n",
        "    \"\"\"Fetch a page and return BeautifulSoup object.\"\"\"\n",
        "    params = {\"name\": letter, \"page\": page}\n",
        "    resp = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    return BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "\n",
        "def fetch_station_table(letter: str, page: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch the stations table for a given starting letter and page number.\n",
        "\n",
        "    Returns a DataFrame with:\n",
        "    ['Station Code', 'Station Name', 'District', 'State', 'Trains passing through']\n",
        "    \"\"\"\n",
        "    soup = _get_soup(letter, page)\n",
        "\n",
        "    container = soup.find(\"div\", class_=\"stationTable\")\n",
        "    if container is None:\n",
        "        # No table for this letter/page\n",
        "        return pd.DataFrame(\n",
        "            columns=[\n",
        "                \"Station Code\",\n",
        "                \"Station Name\",\n",
        "                \"District\",\n",
        "                \"State\",\n",
        "                \"Trains passing through\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    table = container.find(\"table\", class_=\"stn-dir-list-tbl\")\n",
        "    if table is None:\n",
        "        return pd.DataFrame(\n",
        "            columns=[\n",
        "                \"Station Code\",\n",
        "                \"Station Name\",\n",
        "                \"District\",\n",
        "                \"State\",\n",
        "                \"Trains passing through\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Headers\n",
        "    header_row = table.find(\"thead\").find(\"tr\")\n",
        "    headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
        "\n",
        "    # Rows\n",
        "    data_rows = []\n",
        "    tbody = table.find(\"tbody\")\n",
        "    if not tbody:\n",
        "        return pd.DataFrame(columns=headers)\n",
        "\n",
        "    for tr in tbody.find_all(\"tr\"):\n",
        "        cols = tr.find_all(\"td\")\n",
        "        if not cols:\n",
        "            continue\n",
        "\n",
        "        station_code = cols[0].get_text(strip=True)\n",
        "\n",
        "        station_name_cell = cols[1]\n",
        "        station_name_link = station_name_cell.find(\"a\")\n",
        "        station_name = (\n",
        "            station_name_link.get_text(strip=True)\n",
        "            if station_name_link\n",
        "            else station_name_cell.get_text(strip=True)\n",
        "        )\n",
        "\n",
        "        district = cols[2].get_text(strip=True)\n",
        "        state = cols[3].get_text(strip=True)\n",
        "        trains_passing = cols[4].get_text(strip=True)\n",
        "\n",
        "        data_rows.append(\n",
        "            [station_code, station_name, district, state, trains_passing]\n",
        "        )\n",
        "\n",
        "    return pd.DataFrame(data_rows, columns=headers)\n",
        "\n",
        "\n",
        "def _get_last_page_from_soup(soup: BeautifulSoup) -> int:\n",
        "    \"\"\"\n",
        "    Parse the pagination block and return the last page number.\n",
        "    If no pagination or no pages, returns 1.\n",
        "    \"\"\"\n",
        "    pag_div = soup.find(\"div\", class_=\"pagination\")\n",
        "    if not pag_div:\n",
        "        # No pagination, likely only 1 page (or no results)\n",
        "        return 1\n",
        "\n",
        "    # All links in pagination\n",
        "    page_numbers = []\n",
        "    for a in pag_div.find_all(\"a\"):\n",
        "        text = a.get_text(strip=True)\n",
        "        if text.isdigit():\n",
        "            page_numbers.append(int(text))\n",
        "\n",
        "    if not page_numbers:\n",
        "        # Something odd, but fallback to 1\n",
        "        return 1\n",
        "\n",
        "    return max(page_numbers)\n",
        "\n",
        "\n",
        "def fetch_all_stations_for_letter(letter: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch the full stations table for all pages for a given starting letter.\n",
        "    Automatically determines how many pages exist.\n",
        "    \"\"\"\n",
        "    # First, fetch page 1 to determine number of pages\n",
        "    soup_page1 = _get_soup(letter, 1)\n",
        "\n",
        "    # If there's no table at all for this letter, return empty\n",
        "    container = soup_page1.find(\"div\", class_=\"stationTable\")\n",
        "    if container is None:\n",
        "        return pd.DataFrame(\n",
        "            columns=[\n",
        "                \"Station Code\",\n",
        "                \"Station Name\",\n",
        "                \"District\",\n",
        "                \"State\",\n",
        "                \"Trains passing through\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    last_page = _get_last_page_from_soup(soup_page1)\n",
        "\n",
        "    # Fetch page 1 table from the soup we already have\n",
        "    # (avoid double request for page 1)\n",
        "    table_page1 = fetch_station_table(letter, 1)\n",
        "\n",
        "    dfs = [table_page1]\n",
        "\n",
        "    # If only 1 page, we are done\n",
        "    if last_page == 1:\n",
        "        return table_page1.reset_index(drop=True)\n",
        "\n",
        "    # Fetch remaining pages\n",
        "    for page in range(2, last_page + 1):\n",
        "        df_page = fetch_station_table(letter, page)\n",
        "        dfs.append(df_page)\n",
        "\n",
        "    full_df = pd.concat(dfs, ignore_index=True)\n",
        "    return full_df"
      ],
      "metadata": {
        "id": "-bRXqs_jo5xg"
      },
      "id": "-bRXqs_jo5xg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import requests\n",
        "\n",
        "def get_with_backoff(url, params=None, headers=None,\n",
        "                     max_retries=5,\n",
        "                     base_sleep=2.0,\n",
        "                     max_sleep=30.0):\n",
        "    \"\"\"\n",
        "    GET with simple exponential backoff for HTTP 429.\n",
        "    Returns a Response or raises the last error.\n",
        "    \"\"\"\n",
        "    attempt = 0\n",
        "    while True:\n",
        "        try:\n",
        "            resp = requests.get(url, params=params, headers=headers, timeout=30)\n",
        "            if resp.status_code == 429:\n",
        "                # Too many requests: sleep and retry\n",
        "                attempt += 1\n",
        "                if attempt > max_retries:\n",
        "                    resp.raise_for_status()\n",
        "                # Check if server sends Retry-After\n",
        "                retry_after = resp.headers.get(\"Retry-After\")\n",
        "                if retry_after is not None:\n",
        "                    sleep_time = float(retry_after)\n",
        "                else:\n",
        "                    sleep_time = min(max_sleep, base_sleep * (2 ** (attempt - 1)))\n",
        "                # optional jitter\n",
        "                sleep_time += random.uniform(0, 1.0)\n",
        "                print(f\"429 received. Sleeping {sleep_time:.1f}s before retry {attempt}/{max_retries}...\")\n",
        "                time.sleep(sleep_time)\n",
        "                continue\n",
        "\n",
        "            resp.raise_for_status()\n",
        "            return resp\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            attempt += 1\n",
        "            if attempt > max_retries:\n",
        "                raise\n",
        "            sleep_time = min(max_sleep, base_sleep * (2 ** (attempt - 1)))\n",
        "            sleep_time += random.uniform(0, 1.0)\n",
        "            print(f\"Request error: {e}. Sleeping {sleep_time:.1f}s before retry {attempt}/{max_retries}...\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "\n",
        "# Then plug this into your existing code, e.g. inside _get_soup:\n",
        "\n",
        "BASE_URL = \"https://www.railyatri.in/stations\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "        \"(KHTML, like Gecko) Chrome/120.0 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def _get_soup(letter: str, page: int):\n",
        "    params = {\"name\": letter, \"page\": page}\n",
        "    resp = get_with_backoff(BASE_URL, params=params, headers=HEADERS)\n",
        "    from bs4 import BeautifulSoup\n",
        "    return BeautifulSoup(resp.text, \"html.parser\")"
      ],
      "metadata": {
        "id": "QgDToUC2o8YA"
      },
      "id": "QgDToUC2o8YA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.autonotebook import tqdm\n",
        "import time\n",
        "import random\n",
        "\n",
        "dfs = []\n",
        "for letter in tqdm(\n",
        "    letters,\n",
        "    desc=\"Scraping station lists\",\n",
        "    unit=\"letter\",\n",
        "    dynamic_ncols=True,\n",
        "    colour=\"cyan\",\n",
        "    bar_format=\"{l_bar}{bar} {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\",\n",
        "):\n",
        "    df_all = fetch_all_stations_for_letter(letter)\n",
        "    if df_all.shape[0] > 0:\n",
        "        dfs.append(df_all)\n",
        "    # polite pause between letters\n",
        "    time.sleep(1.0 + random.uniform(0, 0.5))"
      ],
      "metadata": {
        "id": "nxN4jyi8rju4"
      },
      "id": "nxN4jyi8rju4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "railway_df = pd.concat(dfs)"
      ],
      "metadata": {
        "id": "199S7dAGsn0T"
      },
      "id": "199S7dAGsn0T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_railway_final = pd.merge(left=df, right=railway_df, left_on=\"station_code\", right_on='Station Code', how=\"left\")\n",
        "df_railway_final.shape"
      ],
      "metadata": {
        "id": "T8-wfXiQtBNW"
      },
      "id": "T8-wfXiQtBNW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_railway_final.to_csv('railway_stations.csv')"
      ],
      "metadata": {
        "id": "1u7Q4xt1tqfo"
      },
      "id": "1u7Q4xt1tqfo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_railway_final['Trains passing through'] = df_railway_final['Trains passing through'].fillna(value=0).astype(int)"
      ],
      "metadata": {
        "id": "JsorBjz3u1UH"
      },
      "id": "JsorBjz3u1UH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_railway_final.sort_values(by='Trains passing through', ascending=False)"
      ],
      "metadata": {
        "id": "hIKSZ_4yuh82"
      },
      "id": "hIKSZ_4yuh82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    data=df_railway_final,  # or any dict/DataFrame with your other attributes\n",
        "    geometry=gpd.points_from_xy(df[\"lon\"], df[\"lat\"]),\n",
        "    crs=\"EPSG:4326\"  # set to your coordinate reference system\n",
        ")\n",
        "gdf = gdf.drop(['lon', \"lat\"], axis=1)\n",
        "\n",
        "gdf.head()"
      ],
      "metadata": {
        "id": "MbXqjsFCbpk-"
      },
      "id": "MbXqjsFCbpk-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf['network'].value_counts()"
      ],
      "metadata": {
        "id": "bikEr2uxcnQA"
      },
      "id": "bikEr2uxcnQA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf.to_file(\"railway_stations.geojson\")"
      ],
      "metadata": {
        "id": "uOeoU9D8cIzT"
      },
      "id": "uOeoU9D8cIzT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}