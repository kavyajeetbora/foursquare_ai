{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPbJOI8MTftCMVhEWD5L/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e5f9209624b04696ba1f9b1dcd73546e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30683aaee6fd46a9a7cf3dc2d512dbc3",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45e3955e5be0404bbe2d936bf52a870d",
            "value": 100
          }
        },
        "30683aaee6fd46a9a7cf3dc2d512dbc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "45e3955e5be0404bbe2d936bf52a870d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "black",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/foursquare_ai/blob/master/notebooks/07_duckdb_ai_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the Python Environment"
      ],
      "metadata": {
        "id": "89XdE0RGkQfo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fs4zr46SQDV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "213fb2eb-2210-4f40-f1af-54842d184ac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/95.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.8/192.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet duckdb jupysql duckdb-engine\n",
        "## Langchain Framework\n",
        "!pip install --quiet langchain langchain-community langchain-openai langgraph \"langchain[openai]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "import os\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import files\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "import json\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# Check if .env file exists, if not, prompt for upload\n",
        "if not os.path.exists(\".env\"):\n",
        "    uploaded = files.upload()\n",
        "    if \".env\" not in uploaded:\n",
        "        print(\"Please upload a .env file to proceed.\")\n",
        "    else:\n",
        "        with open(\".env\", \"wb\") as f:\n",
        "            f.write(uploaded[\".env\"])\n",
        "        load_dotenv(dotenv_path = \".env\", override=True)\n",
        "else:\n",
        "    load_dotenv(dotenv_path = \".env\", override=True)"
      ],
      "metadata": {
        "id": "rcdMclIzTa5j"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"sql_agent_project\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\") # Ensure you have a way to get this"
      ],
      "metadata": {
        "id": "IzeHu2FolbhE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Engineering Part\n",
        "\n",
        "- Here we will prepare the data for our analysis\n",
        "- Clean Data\n",
        "- Load it to local storage for faster loading and querying"
      ],
      "metadata": {
        "id": "bQUQd6DskT1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize DuckDB connection\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Load required extensions\n",
        "con.execute(\"INSTALL httpfs; LOAD httpfs; INSTALL spatial; LOAD spatial;\")\n",
        "\n",
        "s3_places_path = 's3://fsq-os-places-us-east-1/release/dt=2025-09-09/places/parquet/places-*.zstd.parquet'\n",
        "s3_categories_path = 's3://fsq-os-places-us-east-1/release/dt=2025-09-09/categories/parquet/categories.zstd.parquet'\n",
        "\n",
        "# Execute the SELECT query and create a view\n",
        "con.execute(f\"\"\"\n",
        "CREATE OR REPLACE VIEW places_with_categories AS\n",
        "WITH places AS (\n",
        "    SELECT\n",
        "        DISTINCT UNNEST(P.fsq_category_ids) as fsq_category_id,\n",
        "        name,\n",
        "        postcode,\n",
        "        address,\n",
        "        region,\n",
        "        ST_Point(longitude, latitude) AS geom\n",
        "    FROM read_parquet('{s3_places_path}') AS P\n",
        "    WHERE latitude IS NOT NULL AND longitude IS NOT NULL AND country='IN'\n",
        "),\n",
        "places_with_categories AS (\n",
        "    SELECT\n",
        "        P.name AS name,\n",
        "        C.level1_category_name AS category_level_1,\n",
        "        C.level2_category_name AS category_level_2,\n",
        "        postcode,\n",
        "        address,\n",
        "        region,\n",
        "        P.geom\n",
        "    FROM places AS P\n",
        "    JOIN read_parquet('{s3_categories_path}') AS C\n",
        "    ON P.fsq_category_id = C.category_id\n",
        ")\n",
        "SELECT\n",
        "    name,\n",
        "    category_level_1,\n",
        "    category_level_2,\n",
        "    address,\n",
        "    region,\n",
        "    postcode,\n",
        "    geom\n",
        "FROM places_with_categories;\n",
        "\"\"\")\n",
        "\n",
        "# Export the view to GeoParquet\n",
        "con.execute(\"COPY (SELECT * FROM places_with_categories) TO 'output.geoparquet' WITH (FORMAT PARQUET, CODEC ZSTD);\")\n",
        "\n",
        "## Check the total count of the database:\n",
        "# con.execute(\"SELECT COUNT(*) FROM places_with_categories;\")\n",
        "# result = con.fetchone()[0]\n",
        "# print(result)\n",
        "## Around 1358392 points are there\n",
        "\n",
        "# Close the connection\n",
        "con.close()"
      ],
      "metadata": {
        "id": "ldsOwjrka_mc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e5f9209624b04696ba1f9b1dcd73546e",
            "30683aaee6fd46a9a7cf3dc2d512dbc3",
            "45e3955e5be0404bbe2d936bf52a870d"
          ]
        },
        "outputId": "984084d1-f224-4d25-9eef-c0adb310bfc5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5f9209624b04696ba1f9b1dcd73546e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI ChatBot\n",
        "\n",
        "-"
      ],
      "metadata": {
        "id": "uFGrjdtDgnhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Prompt Templates\n",
        "\n",
        "- System prompts"
      ],
      "metadata": {
        "id": "rcKMP_4fgUlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"\n",
        "Given an input question, create a syntactically correct DuckDB's SQL dialect query to\n",
        "run to help find the answer. Unless the user specifies in his question a\n",
        "specific number of examples they wish to obtain, always limit your query to\n",
        "at most {top_k} results. You can order the results by a relevant column to\n",
        "return the most interesting examples in the database.\n",
        "\n",
        "Never query for all the columns from a specific table, only ask for a the\n",
        "few relevant columns given the question.\n",
        "\n",
        "Pay attention to use only the column names that you can see in the schema\n",
        "description. Be careful to not query for columns that do not exist. Also,\n",
        "pay attention to which column is in which table.\n",
        "\n",
        "Only use the following tables:\n",
        "{table_info}\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = \"Question: {input}\"\n",
        "\n",
        "query_prompt_template = ChatPromptTemplate(\n",
        "    [(\"system\", system_message), (\"user\", user_prompt)]\n",
        ")\n",
        "\n",
        "for message in query_prompt_template.messages:\n",
        "    message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRrOoj4xgY7k",
        "outputId": "e0af4b3e-35b9-408e-f03a-9328fdffdd6c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "\n",
            "Given an input question, create a syntactically correct DuckDB's SQL dialect query to\n",
            "run to help find the answer. Unless the user specifies in his question a\n",
            "specific number of examples they wish to obtain, always limit your query to\n",
            "at most \u001b[33;1m\u001b[1;3m{top_k}\u001b[0m results. You can order the results by a relevant column to\n",
            "return the most interesting examples in the database.\n",
            "\n",
            "Never query for all the columns from a specific table, only ask for a the\n",
            "few relevant columns given the question.\n",
            "\n",
            "Pay attention to use only the column names that you can see in the schema\n",
            "description. Be careful to not query for columns that do not exist. Also,\n",
            "pay attention to which column is in which table.\n",
            "\n",
            "Only use the following tables:\n",
            "\u001b[33;1m\u001b[1;3m{table_info}\u001b[0m\n",
            "\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Question: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions\n",
        "\n",
        "- Setting up connection with duckdb\n",
        "- Then generating shema details for all the tables in the database\n",
        "- Define the input and the output structure format for all the parameters are going to be passed on to the llm"
      ],
      "metadata": {
        "id": "4HwEcFaqj0-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pydantic Models with @field_validator\n",
        "class State(BaseModel):\n",
        "    question: str = Field(\n",
        "        default=\"\",\n",
        "        description=\"The natural language question or input provided by the user, intended to be translated into a DuckDB SQL query. Example: 'Show me all customers with orders above $100.'\"\n",
        "    )\n",
        "    query: str = Field(\n",
        "        default=\"\",\n",
        "        description=\"The generated DuckDB SQL query derived from the user's natural language question. Must conform to DuckDB's SQL syntax and conventions. Example: 'SELECT * FROM customers WHERE order_total > 100;'\"\n",
        "    )\n",
        "    result: str = Field(\n",
        "        default=\"\",\n",
        "        description=\"The output or result set returned by executing the DuckDB SQL query against the database, formatted as a JSON string. Example: '[{\\\"id\\\": 1, \\\"name\\\": \\\"Alice\\\", \\\"order_total\\\": 150}]'\"\n",
        "    )\n",
        "    answer: str = Field(\n",
        "        default=\"\",\n",
        "        description=\"The final natural language response generated for the user, summarizing or explaining the query results in a conversational manner. Example: 'Here are the customers with orders above $100: Alice with $150.'\"\n",
        "    )\n",
        "\n",
        "    @field_validator(\"result\", mode=\"after\")\n",
        "    @classmethod\n",
        "    def validate_result(cls, result: str) -> str:\n",
        "        try:\n",
        "            json.loads(result)\n",
        "        except json.JSONDecodeError:\n",
        "            raise ValueError(\"Result must be a valid JSON string\")\n",
        "        return result\n",
        "\n",
        "class QueryOutput(BaseModel):\n",
        "    query: str = Field(..., description=\"Syntactically valid DuckDB SQL query\")\n",
        "\n",
        "    @field_validator(\"query\", mode=\"after\")\n",
        "    @classmethod\n",
        "    def validate_query(cls, query: str) -> str:\n",
        "        if not query.strip().endswith(\";\"):\n",
        "            raise ValueError(\"DuckDB query must end with a semicolon\")\n",
        "        return query\n",
        "\n",
        "# Prompt Setup (unchanged)\n",
        "system_message = \"\"\"\n",
        "Given an input question, create a syntactically correct DuckDB's SQL dialect query to\n",
        "run to help find the answer. Unless the user specifies in his question a\n",
        "specific number of examples they wish to obtain, always limit your query to\n",
        "at most {top_k} results. You can order the results by a relevant column to\n",
        "return the most interesting examples in the database.\n",
        "\n",
        "Never query for all the columns from a specific table, only ask for the\n",
        "few relevant columns given the question.\n",
        "\n",
        "Pay attention to use only the column names that you can see in the schema\n",
        "description. Be careful to not query for columns that do not exist. Also,\n",
        "pay attention to which column is in which table.\n",
        "\n",
        "Only use the following tables:\n",
        "{table_info}\n",
        "\"\"\"\n",
        "user_prompt = \"Question: {input}\"\n",
        "query_prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"user\", user_prompt)])\n",
        "\n",
        "class FourSquareChatBot:\n",
        "    \"\"\"NLP-to-SQL chatbot for querying DuckDB databases, optimized for Parquet files and FourSquare data.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, columns: list[str], llm, database: str = \":memory:\"):\n",
        "        \"\"\"\n",
        "        Initialize the chatbot with a DuckDB connection and schema.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the Parquet file (e.g., S3 URL or local path).\n",
        "            columns (list[str]): List of column names to include in the schema.\n",
        "            llm: Language model instance for generating SQL queries and answers.\n",
        "            database (str): DuckDB database path (default: ':memory:' for in-memory).\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.columns = columns\n",
        "        self.llm = llm\n",
        "        self.conn = self._get_duckdb_connection(database)\n",
        "        self.table_info = self._get_db_schema(limit=5)\n",
        "\n",
        "    def _get_duckdb_connection(self, database: str) -> duckdb.DuckDBPyConnection:\n",
        "        \"\"\"Create and configure a DuckDB connection.\"\"\"\n",
        "        con = duckdb.connect(database=database)\n",
        "        con.execute(\"INSTALL httpfs;\")\n",
        "        con.execute(\"LOAD httpfs;\")\n",
        "        con.execute(\"INSTALL spatial;\")\n",
        "        con.execute(\"LOAD spatial;\")\n",
        "        # Optional: Set S3 region if needed\n",
        "        # con.execute(\"SET s3_region='us-east-1';\")\n",
        "        return con\n",
        "\n",
        "    def _get_db_schema(self, limit=5):\n",
        "\n",
        "        \"\"\"Generate schema information from the Parquet file with sample values.\"\"\"\n",
        "\n",
        "        data_schema = f\"\"\"Columns:\\n\"\"\"\n",
        "\n",
        "        sql_query = f\"SELECT {\",\".join(self.columns)} FROM read_parquet('{self.data_path}') WHERE 1=1\"\n",
        "\n",
        "        for column in self.columns:\n",
        "            sql_query += f\" AND {column} IS NOT NULL\"\n",
        "\n",
        "        sql_query += f\" LIMIT {limit};\"\n",
        "        sample_result = self._execute_sql(sql_query)\n",
        "\n",
        "        schema_details = self._execute_sql(f'DESCRIBE {sql_query}')\n",
        "\n",
        "        for i, column in enumerate(self.columns):\n",
        "            data_type = schema_details[i][1]\n",
        "            sample_values = \",\".join([str(r[i]) for r in sample_result])\n",
        "            data_schema += f\"{i+1}. Name: {column} | Data Type: {data_type} | Sample values: {sample_values}\"\n",
        "            data_schema += \"\\n\"\n",
        "        return data_schema\n",
        "\n",
        "    def _execute_sql(self, sql_query: str) -> list:\n",
        "        \"\"\"Execute a SQL query and return results.\"\"\"\n",
        "        try:\n",
        "            result = self.conn.execute(sql_query).fetchall()\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return f\"Error executing SQL: {str(e)}\"\n",
        "\n",
        "    def generate_sql_query(self, state: State) -> QueryOutput:\n",
        "        \"\"\"Generate a DuckDB SQL query from the user's question.\"\"\"\n",
        "        prompt = query_prompt_template.invoke(\n",
        "            {\n",
        "                \"top_k\": 10,\n",
        "                \"table_info\": self.table_info,\n",
        "                \"input\": state.question\n",
        "            }\n",
        "        )\n",
        "        response = self.llm.invoke(prompt)\n",
        "        return QueryOutput(query=response.content)\n",
        "\n",
        "    def generate_answer(self, state: State) -> dict:\n",
        "        \"\"\"Generate a conversational answer using query results.\"\"\"\n",
        "        # Execute the query and store JSON result\n",
        "        result = self._execute_sql(state.query)\n",
        "        if isinstance(result, str) and result.startswith(\"Error\"):\n",
        "            state.result = json.dumps({\"error\": result})\n",
        "            state.answer = f\"Sorry, I couldn't process your query due to an error: {result}\"\n",
        "        else:\n",
        "            columns = [desc[0] for desc in self.conn.description]\n",
        "            state.result = json.dumps([dict(zip(columns, row)) for row in result])\n",
        "\n",
        "            # Generate conversational answer\n",
        "            prompt = (\n",
        "                \"Given the following user question, corresponding SQL query, \"\n",
        "                \"and SQL result, answer the user question in a conversational manner.\\n\\n\"\n",
        "                f\"Question: {state.question}\\n\"\n",
        "                f\"SQL Query: {state.query}\\n\"\n",
        "                f\"SQL Result: {state.result}\"\n",
        "            )\n",
        "            response = self.llm.invoke(prompt)\n",
        "            state.answer = response.content\n",
        "\n",
        "        return {\"state\": state}\n",
        "\n",
        "    def process_question(self, question: str) -> dict:\n",
        "        \"\"\"Process a user question end-to-end and return the updated state.\"\"\"\n",
        "        state = State(question=question)\n",
        "        query_output = self.generate_sql_query(state)\n",
        "        state.query = query_output.query\n",
        "        result = self.generate_answer(state)\n",
        "        return result\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Clean up by closing the DuckDB connection.\"\"\"\n",
        "        if hasattr(self, \"conn\"):\n",
        "            self.conn.close()"
      ],
      "metadata": {
        "id": "yjktwyoHdzoZ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "fsq_chat_bot = FourSquareChatBot(\n",
        "    data_path = \"/content/output.geoparquet\",\n",
        "    columns = ['name', 'category_level_1', 'category_level_2', 'address', 'region', 'postcode'],\n",
        "    llm = llm\n",
        ")"
      ],
      "metadata": {
        "id": "ZrJYpyPuxbH8",
        "outputId": "89518e51-7479-4504-b18f-799e898df78b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VARCHAR\n",
            "VARCHAR\n",
            "VARCHAR\n",
            "VARCHAR\n",
            "VARCHAR\n",
            "VARCHAR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fsq_chat_bot.table_info)"
      ],
      "metadata": {
        "id": "vwHtHUG_xzFq",
        "outputId": "750c6668-5843-41cd-bc92-3c0e4c805dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns:\n",
            "1. Name: name | Data Type: VARCHAR | Sample values: Indane - Boham Gramin Vitrak,Gulf Carstop - Gogoi Automobile,Indane - Wangcha Gas Agency,Axis Bank,Axis Bank ATM\n",
            "2. Name: category_level_1 | Data Type: VARCHAR | Sample values: Travel and Transportation,Retail,Travel and Transportation,Business and Professional Services,Business and Professional Services\n",
            "3. Name: category_level_2 | Data Type: VARCHAR | Sample values: Fuel Station,Automotive Retail,Fuel Station,Financial Service,Financial Service\n",
            "4. Name: address | Data Type: VARCHAR | Sample values: Ground Floor,Ground Floor,Bagh Moria Gaon,Ground Floor,1st Floor, NH 52,1st Floor,NH 52\n",
            "5. Name: region | Data Type: VARCHAR | Sample values: Arunachal Pradesh,Assam,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh\n",
            "6. Name: postcode | Data Type: VARCHAR | Sample values: 792130,785692,792131,791110,791110\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FourSquareChatBot:\n",
        "\n",
        "\n",
        "def get_duckdb_connection():\n",
        "    con = duckdb.connect(database=':memory:')  # In-memory for simplicity; use a file path for persistence if needed\n",
        "    con.execute(\"INSTALL httpfs;\")\n",
        "    con.execute(\"LOAD httpfs;\")\n",
        "    con.execute(\"INSTALL spatial;\")\n",
        "    con.execute(\"LOAD spatial;\")\n",
        "    # Optional: Set S3 region if needed (public bucket, so usually not required)\n",
        "    # con.execute(\"SET s3_region='us-east-1';\")\n",
        "    return con\n",
        "\n",
        "def get_db_schema(DATA_PATH, columns, duckdb_conn, limit=5):\n",
        "\n",
        "    data_schema = f\"\"\"Columns:\\n\"\"\"\n",
        "\n",
        "    sql_query = f\"SELECT {\",\".join(columns)} FROM read_parquet('{DATA_PATH}') WHERE 1=1\"\n",
        "\n",
        "    for column in columns:\n",
        "        sql_query += f\" AND {column} IS NOT NULL\"\n",
        "\n",
        "    sql_query += f\" LIMIT {limit};\"\n",
        "    sample_result = execute_sql(sql_query, duckdb_conn)\n",
        "\n",
        "    schema_details = execute_sql(f'DESCRIBE {sql_query}', duckdb_conn)\n",
        "\n",
        "    for i, column in enumerate(columns):\n",
        "        data_type = schema_details[i][1]\n",
        "        sample_values = \",\".join([str(r[i]) for r in sample_result])\n",
        "        data_schema += f\"{i+1}. Name: {column} | Data Type: {data_type} | Sample values: {sample_values}\"\n",
        "        data_schema += \"\\n\"\n",
        "    return data_schema\n",
        "\n",
        "\n",
        "def execute_sql(sql_query: str, con):\n",
        "    try:\n",
        "        # Replace {s3_path} if not already in the query\n",
        "        result = con.execute(sql_query).fetchall()\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"Error executing SQL: {str(e)}\"\n",
        "\n",
        "\n",
        "def generate_sql_query(state: State, table_info: str) -> QueryOutput:\n",
        "    \"Generate SQL Query to fetch information\"\n",
        "    prompt = query_prompt_template.invoke(\n",
        "        {\n",
        "            \"top_k\": 10,\n",
        "            \"table_info\": table_info,\n",
        "            \"input\": state.question\n",
        "        }\n",
        "    )\n",
        "    response = llm.invoke(prompt)\n",
        "    return response\n",
        "\n",
        "# Generate answer (already correct)\n",
        "def generate_answer(state: State):\n",
        "    \"\"\"Answer question using retrieved information as context.\"\"\"\n",
        "    prompt = (\n",
        "        \"Given the following user question, corresponding SQL query, \"\n",
        "        \"and SQL result, answer the user question.\\n\\n\"\n",
        "        f\"Question: {state.question}\\n\"\n",
        "        f\"SQL Query: {state.query}\\n\"\n",
        "        f\"SQL Result: {state.result}\"\n",
        "    )\n",
        "    response = llm.invoke(prompt)\n",
        "    return {\"answer\": response.content}"
      ],
      "metadata": {
        "id": "WYQkUDLBhFHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564aa45c-4ac3-4b6a-e173-7a4de7e4dd00"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table: data\n",
            "Columns:\n",
            "1. Name: name | Data Type: name | Sample values: Indane - Boham Gramin Vitrak,Gulf Carstop - Gogoi Automobile,Indane - Wangcha Gas Agency,Axis Bank,Axis Bank ATM\n",
            "2. Name: category_level_1 | Data Type: category_level_1 | Sample values: Travel and Transportation,Retail,Travel and Transportation,Business and Professional Services,Business and Professional Services\n",
            "3. Name: category_level_2 | Data Type: category_level_2 | Sample values: Fuel Station,Automotive Retail,Fuel Station,Financial Service,Financial Service\n",
            "4. Name: address | Data Type: address | Sample values: Ground Floor,Ground Floor,Bagh Moria Gaon,Ground Floor,1st Floor, NH 52,1st Floor,NH 52\n",
            "5. Name: region | Data Type: region | Sample values: Arunachal Pradesh,Assam,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh\n",
            "6. Name: postcode | Data Type: postcode | Sample values: 792130,785692,792131,791110,791110\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "duckdb_conn = get_duckdb_connection()\n",
        "\n",
        "DATA_PATH = \"/content/output.geoparquet\"\n",
        "columns = ['name', 'category_level_1', 'category_level_2', 'address', 'region', 'postcode']\n",
        "\n",
        "table_info = get_db_schema(DATA_PATH, columns, duckdb_conn, limit=10)\n",
        "print(table_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLr9wuFJumDQ",
        "outputId": "81eea899-bdce-473f-f108-5015e1b73730"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns:\n",
            "1. Name: name | Data Type: VARCHAR | Sample values: Indane - Boham Gramin Vitrak,Gulf Carstop - Gogoi Automobile,Indane - Wangcha Gas Agency,Axis Bank,Axis Bank ATM,HDFC ERGO Insurance Agent: Partha P Bhattacharjee,HDFC ERGO Insurance Agent: Mintu Roy,HDFC Bank ATM,HDFC Bank,Punjab National Bank\n",
            "2. Name: category_level_1 | Data Type: VARCHAR | Sample values: Travel and Transportation,Retail,Travel and Transportation,Business and Professional Services,Business and Professional Services,Business and Professional Services,Business and Professional Services,Business and Professional Services,Business and Professional Services,Business and Professional Services\n",
            "3. Name: category_level_2 | Data Type: VARCHAR | Sample values: Fuel Station,Automotive Retail,Fuel Station,Financial Service,Financial Service,Financial Service,Financial Service,Financial Service,Financial Service,Financial Service\n",
            "4. Name: address | Data Type: VARCHAR | Sample values: Ground Floor,Ground Floor,Bagh Moria Gaon,Ground Floor,1st Floor, NH 52,1st Floor,NH 52,No E/19,Ground Floor,Roing Circle,Sindu Complex,Sindu Cplx,Pine Plaza,Lower Market\n",
            "5. Name: region | Data Type: VARCHAR | Sample values: Arunachal Pradesh,Assam,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh\n",
            "6. Name: postcode | Data Type: VARCHAR | Sample values: 792130,785692,792131,791110,791110,792110,792110,792110,792110,792110\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph, END\n",
        "from IPython.display import Image, display\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_sequence([generate_sql_query])\n",
        "graph_builder.add_edge(START, \"generate_sql_query\")\n",
        "# graph_builder.add_edge(\"generate_answer\", END)\n",
        "graph = graph_builder.compile()\n"
      ],
      "metadata": {
        "id": "YNewyIgctv_F"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Tell me the count for all the points that are related to manufacturing?\"\n",
        "for step in graph.stream(\n",
        "    {\n",
        "        \"question\": question,\n",
        "        \"query\": \"\",\n",
        "        \"result\": \"\",\n",
        "        \"answer\": \"\",\n",
        "        \"table_info\": table_info # Add table_info here\n",
        "    },\n",
        "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
        "    stream_mode=\"updates\"\n",
        "\n",
        "):\n",
        "    print(step)\n",
        "\n",
        "print(\"-\"*100,\"\\n\")\n",
        "print(step['generate_sql_query'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "z6ACIyJSuSoC",
        "outputId": "851c46d3-137c-4764-e94a-0ed889824c72"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "generate_sql_query() missing 1 required positional argument: 'table_info'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-492628235.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Tell me the count for all the points that are related to manufacturing?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m for step in graph.stream(\n\u001b[0m\u001b[1;32m      3\u001b[0m     {\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2655\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2658\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: generate_sql_query() missing 1 required positional argument: 'table_info'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BsiVwNy4hXLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a76f922-1863-485d-97c2-afcc87eabe96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns:\n",
            "1. Name: name | Data Type: VARCHAR | Sample values: Indane - Boham Gramin Vitrak,Gulf Carstop - Gogoi Automobile,Indane - Wangcha Gas Agency,Axis Bank,Axis Bank ATM,HDFC ERGO Insurance Agent: Partha P Bhattacharjee,HDFC ERGO Insurance Agent: Mintu Roy,HDFC Bank ATM,HDFC Bank,Punjab National Bank\n",
            "2. Name: category_level_1 | Data Type: VARCHAR | Sample values: Travel and Transportation,Retail,Travel and Transportation,Business and Professional Services,Business and Professional Services,Business and Professional Services,Business and Professional Services,Business and Professional Services,Business and Professional Services,Business and Professional Services\n",
            "3. Name: category_level_2 | Data Type: VARCHAR | Sample values: Fuel Station,Automotive Retail,Fuel Station,Financial Service,Financial Service,Financial Service,Financial Service,Financial Service,Financial Service,Financial Service\n",
            "4. Name: address | Data Type: VARCHAR | Sample values: Ground Floor,Ground Floor,Bagh Moria Gaon,Ground Floor,1st Floor, NH 52,1st Floor,NH 52,No E/19,Ground Floor,Roing Circle,Sindu Complex,Sindu Cplx,Pine Plaza,Lower Market\n",
            "5. Name: region | Data Type: VARCHAR | Sample values: Arunachal Pradesh,Assam,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh,Arunachal Pradesh\n",
            "6. Name: postcode | Data Type: VARCHAR | Sample values: 792130,785692,792131,791110,791110,792110,792110,792110,792110,792110\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the chatbot"
      ],
      "metadata": {
        "id": "a4jyhz_Ijwiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Pre-prompt for NL-to-SQL (customize this!)\n",
        "# Updated prompt to include required input variables for create_sql_query_chain\n",
        "# and incorporate the S3_PATH as a partial variable\n",
        "SQL_PROMPT_TEMPLATE = \"\"\"\n",
        "You are a SQL expert for querying Foursquare POI data using DuckDB. The data is in Parquet files at {s3_path}.\n",
        "\n",
        "Based on the schema below and the user's question, generate a valid DuckDB SQL query.\n",
        "Schema:\n",
        "{table_info}\n",
        "\n",
        "Contraints:\n",
        "Always restrict the oulets aka POI withing INDIA.\n",
        "If any other country is asked by the uers just say something like beyond scope and donot generate any sql prompt\"\n",
        "\n",
        "User question: {input}\n",
        "Limit the results to {top_k}.\n",
        "\n",
        "Generate a valid DuckDB SQL query to answer this. Always use read_parquet('{s3_path}') as the data source.\n",
        "Do not add explanations; output only the SQL query.\n",
        "Examples:\n",
        "- Question: How many Starbucks Outlets are there in India?\n",
        "  SQL: SELECT COUNT(*) as count FROM read_parquet('{s3_path}') WHERE LOWER(name) LIKE '%starbucks%' OR '%starbucks%' IN (SELECT LOWER(unnest(fsq_category_labels))));\n",
        "\"\"\"\n",
        "\n",
        "response_prompt = PromptTemplate.from_template(RESPONSE_TEMPLATE)\n",
        "\n",
        "# Create the prompt template with the required input variables and partial variable for S3_PATH\n",
        "sql_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"top_k\", \"table_info\"],\n",
        "    template=SQL_PROMPT_TEMPLATE,\n",
        "    partial_variables={\"s3_path\": DATA_PATH}\n",
        ")\n",
        "\n",
        "chain = sql_prompt | llm | StrOutputParser()\n",
        "\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"input\": \"Show me the number of outlets in Gurgaon area?\",\n",
        "        \"top_k\": 10,\n",
        "        \"table_info\": table_info\n",
        "    }\n",
        ")\n",
        "\n",
        "response_chain = response_prompt | llm\n",
        "\n",
        "response_chain\n",
        "\n",
        "# Execute the generated SQL query\n",
        "query_result = execute_sql(response, duckdb_conn)\n",
        "print(query_result)"
      ],
      "metadata": {
        "id": "erVSH0uIjv7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76e8a50d-18f7-46ac-d8e8-a57b7316fc70"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SELECT COUNT(*) as count \n",
            "FROM read_parquet('/content/output.geoparquet') \n",
            "WHERE LOWER(address) LIKE '%gurgaon%' \n",
            "LIMIT 10;\n",
            "\n",
            "[(1097,)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke(\n",
        "    {\n",
        "        \"input\": \"give me the number of outlets with category retail?\",\n",
        "        \"top_k\": 10,\n",
        "        \"table_info\": table_info\n",
        "    }\n",
        ")\n",
        "\n",
        "response = response.replace('```','').replace('sql','')\n",
        "print(response)\n",
        "\n",
        "# Execute the generated SQL query\n",
        "query_result = execute_sql(response, duckdb_conn)\n",
        "print(query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LAuuO7FXeWo",
        "outputId": "65dac2a7-83a5-45a0-9752-5c17be02338d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SELECT COUNT(*) as count FROM read_parquet('/content/output.geoparquet') WHERE category_level_1 = 'Retail' LIMIT 10;\n",
            "\n",
            "[(292123,)]\n"
          ]
        }
      ]
    }
  ]
}