{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGvFm2BWxUDxGFMyRdSKfb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyajeetbora/foursquare_ai/blob/master/notebooks/07_duckdb_ai_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the Python Environment"
      ],
      "metadata": {
        "id": "89XdE0RGkQfo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs4zr46SQDV9"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet duckdb jupysql duckdb-engine\n",
        "## Langchain Framework\n",
        "!pip install --quiet langchain langchain-community langchain-openai langgraph \"langchain[openai]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "import os\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import files\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "import json\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# Check if .env file exists, if not, prompt for upload\n",
        "if not os.path.exists(\".env\"):\n",
        "    uploaded = files.upload()\n",
        "    if \".env\" not in uploaded:\n",
        "        print(\"Please upload a .env file to proceed.\")\n",
        "    else:\n",
        "        with open(\".env\", \"wb\") as f:\n",
        "            f.write(uploaded[\".env\"])\n",
        "        load_dotenv(dotenv_path = \".env\", override=True)\n",
        "else:\n",
        "    load_dotenv(dotenv_path = \".env\", override=True)"
      ],
      "metadata": {
        "id": "rcdMclIzTa5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"sql_agent_project\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\") # Ensure you have a way to get this"
      ],
      "metadata": {
        "id": "IzeHu2FolbhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Engineering Part\n",
        "\n",
        "- Here we will prepare the data for our analysis\n",
        "- Clean Data\n",
        "- Load it to local storage for faster loading and querying"
      ],
      "metadata": {
        "id": "bQUQd6DskT1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize DuckDB connection\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Load required extensions\n",
        "con.execute(\"INSTALL httpfs; LOAD httpfs; INSTALL spatial; LOAD spatial;\")\n",
        "\n",
        "s3_places_path = 's3://fsq-os-places-us-east-1/release/dt=2025-09-09/places/parquet/places-*.zstd.parquet'\n",
        "s3_categories_path = 's3://fsq-os-places-us-east-1/release/dt=2025-09-09/categories/parquet/categories.zstd.parquet'\n",
        "\n",
        "# Execute the SELECT query and create a view\n",
        "con.execute(f\"\"\"\n",
        "CREATE OR REPLACE VIEW places_with_categories AS\n",
        "WITH places AS (\n",
        "    SELECT\n",
        "        DISTINCT UNNEST(P.fsq_category_ids) as fsq_category_id,\n",
        "        name,\n",
        "        postcode,\n",
        "        address,\n",
        "        region,\n",
        "        ST_Point(longitude, latitude) AS geom\n",
        "    FROM read_parquet('{s3_places_path}') AS P\n",
        "    WHERE latitude IS NOT NULL AND longitude IS NOT NULL AND country='IN'\n",
        "),\n",
        "places_with_categories AS (\n",
        "    SELECT\n",
        "        P.name AS name,\n",
        "        C.level1_category_name AS category_level_1,\n",
        "        C.level2_category_name AS category_level_2,\n",
        "        postcode,\n",
        "        address,\n",
        "        region,\n",
        "        P.geom\n",
        "    FROM places AS P\n",
        "    JOIN read_parquet('{s3_categories_path}') AS C\n",
        "    ON P.fsq_category_id = C.category_id\n",
        ")\n",
        "SELECT\n",
        "    name,\n",
        "    category_level_1,\n",
        "    category_level_2,\n",
        "    address,\n",
        "    region,\n",
        "    postcode,\n",
        "    geom\n",
        "FROM places_with_categories;\n",
        "\"\"\")\n",
        "\n",
        "# Export the view to GeoParquet\n",
        "con.execute(\"COPY (SELECT * FROM places_with_categories) TO 'output.geoparquet' WITH (FORMAT PARQUET, CODEC ZSTD);\")\n",
        "\n",
        "## Check the total count of the database:\n",
        "# con.execute(\"SELECT COUNT(*) FROM places_with_categories;\")\n",
        "# result = con.fetchone()[0]\n",
        "# print(result)\n",
        "## Around 1358392 points are there\n",
        "\n",
        "# Close the connection\n",
        "con.close()"
      ],
      "metadata": {
        "id": "ldsOwjrka_mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Check the total count of the database:\n",
        "\n",
        "example_query = '''\n",
        "SELECT COUNT(*) as count FROM read_parquet('/content/output.geoparquet') WHERE (LOWER(name) LIKE '%manufacturing%' OR LOWER(category_level_1) LIKE '%manufacturing%' OR LOWER(category_level_2) LIKE '%manufacturing%' OR LOWER(address) LIKE '%manufacturing%') LIMIT 10;\n",
        "'''\n",
        "# example_query = '''\n",
        "# SELECT name, category_level_1, category_level_2, address, region, postcode\n",
        "# FROM read_parquet('/content/output.geoparquet')\n",
        "# WHERE (LOWER(name) LIKE '%coffee%'\n",
        "# OR LOWER(category_level_1) LIKE '%coffee%'\n",
        "# OR LOWER(category_level_2) LIKE '%coffee%'\n",
        "# OR LOWER(address) LIKE '%coffee%')\n",
        "# AND (LOWER(region) LIKE '%bangalore%' OR LOWER(address) LIKE '%bangalore%')\n",
        "# ORDER BY CASE\n",
        "# WHEN LOWER(name) LIKE '%coffee%' THEN 1\n",
        "# WHEN LOWER(category_level_2) LIKE '%coffee%' THEN 2\n",
        "# WHEN LOWER(category_level_1) LIKE '%coffee%' THEN 3\n",
        "# ELSE 4 END\n",
        "# LIMIT 10;\n",
        "# '''\n",
        "\n",
        "con = duckdb.connect()\n",
        "result = con.execute(example_query).df()\n",
        "con.close()\n",
        "## Around 1358392 points are there\n",
        "\n",
        "result.head()"
      ],
      "metadata": {
        "id": "2cSgzwcpZn3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Chatbot for DuckDB Queries: Concise Overview\n",
        "\n",
        "- **Core Objective**: Build an AI chatbot that converts natural language questions into DuckDB SQL queries on Parquet data (e.g., POI datasets), executes them, and generates conversational responses.\n",
        "- **Tech Stack**: OpenAI LLM (GPT-4o-mini) for SQL/answer generation; LangGraph for stateful workflow orchestration; DuckDB for efficient Parquet querying.\n",
        "- **Pipeline Flow**:\n",
        "    1. LLM generates SQL from question + schema;\n",
        "    2. Execute query and format results as JSON;\n",
        "    3. LLM synthesizes witty, concise answer from question, SQL, and results.\n",
        "    \n",
        "- **Key Implementation**: Pydantic `State` for propagation; Graph nodes for gen/execute/answer with linear edges; Auto-schema from Parquet (columns/types/samples) for prompts.\n",
        "- **Usage & Next**: Invoke via `graph.invoke(State(question=\"...\"))`; Extend with geospatial filters, multi-turn history, or error retries."
      ],
      "metadata": {
        "id": "uFGrjdtDgnhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Input and Output Structured Format\n"
      ],
      "metadata": {
        "id": "rcKMP_4fgUlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class State(BaseModel):\n",
        "    question: str = Field(\n",
        "        default=\"\",\n",
        "        description=\"The natural language question or input provided by the user, intended to be translated into a DuckDB SQL query. Example: 'Show me all customers with orders above $100.'\"\n",
        "    )\n",
        "    data_path: str = Field(\n",
        "        default=\"\",\n",
        "        description=\"This is the data source of the table from where we will be querying using duckdb. This is a parquet file\"\n",
        "    )\n",
        "    query: str = Field(\n",
        "        default=\"\",\n",
        "        description=\"The generated DuckDB SQL query derived from the user's natural language question. Must conform to DuckDB's SQL syntax and conventions. Example: 'SELECT * FROM customers WHERE order_total > 100;'\"\n",
        "    )\n",
        "    result: str = Field(\n",
        "        default=\"\",\n",
        "        description=\"The output or result set returned by executing the DuckDB SQL query against the database, formatted as a JSON string. Example: '[{\\\"id\\\": 1, \\\"name\\\": \\\"Alice\\\", \\\"order_total\\\": 150}]'\"\n",
        "    )\n",
        "    answer: str = Field(\n",
        "        default=\"\",\n",
        "        description=\"The final natural language response generated for the user, summarizing or explaining the query results in a conversational manner. Example: 'Here are the customers with orders above $100: Alice with $150.'\"\n",
        "    )\n",
        "\n",
        "    @field_validator(\"result\", mode=\"after\")\n",
        "    @classmethod\n",
        "    def validate_result(cls, result: str) -> str:\n",
        "        try:\n",
        "            json.loads(result)\n",
        "        except json.JSONDecodeError:\n",
        "            raise ValueError(\"Result must be a valid JSON string\")\n",
        "        return result\n",
        "\n",
        "class QueryOutput(BaseModel):\n",
        "    query: str = Field(..., description=\"Syntactically valid DuckDB SQL query\")\n",
        "\n",
        "    @field_validator(\"query\", mode=\"after\")\n",
        "    @classmethod\n",
        "    def validate_query(cls, query: str) -> str:\n",
        "        if not query.strip().endswith(\";\"):\n",
        "            raise ValueError(\"DuckDB query must end with a semicolon\")\n",
        "        return query\n"
      ],
      "metadata": {
        "id": "iFWmhfuzSCDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions\n",
        "\n",
        "- Setting up connection with duckdb\n",
        "- Then generating shema details for all the tables in the database\n",
        "- Define the input and the output structure format for all the parameters are going to be passed on to the llm"
      ],
      "metadata": {
        "id": "4HwEcFaqj0-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the system prompt template for the AI chat bot:"
      ],
      "metadata": {
        "id": "Oy4sIjEcSHs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Setup (unchanged)\n",
        "system_message = \"\"\"\n",
        "Given an input question, create a syntactically correct DuckDB SQL query to answer it using data from Parquet files at {data_path}.\n",
        "Limit results to {top_k} unless the user specifies a different number, ordering by a relevant column for the most interesting results.\n",
        "Use only columns: `name`, `category_level_1`, `category_level_2`, `address`, `region`, `postcode`.\n",
        "For keyword searches (e.g., \"temple,\" \"hotel\"),\n",
        "search across `name`, `category_level_1`, `category_level_2`, and `address` using LOWER() and LIKE, prioritizing matches in `name` and `category_level_2` via ORDER BY CASE.\n",
        "For region-specific queries, check `region` and `address` columns.\n",
        "\n",
        "Restrict outlets (POIs) to India only.\n",
        "If a non-India country is mentioned, respond with \"Query beyond scope, restricted to India\" and do not generate a query.\n",
        "\n",
        "Use read_parquet('{data_path}') as the data source.\n",
        "\n",
        "Schema: {table_info}\n",
        "User question: {input}\n",
        "Limit: {top_k}\n",
        "Output only the Duckdb query without explanations.\n",
        "Strictly stick to the above mentioned schema only.\n",
        "\n",
        "Few Shot Examples:\n",
        "- Question: How many temples are there in India?\n",
        "- Query: SELECT COUNT(*) as count FROM read_parquet('{data_path}') WHERE (LOWER(name) LIKE '%temple%' OR LOWER(category_level_1) LIKE '%temple%' OR LOWER(category_level_2) LIKE '%temple%' OR LOWER(address) LIKE '%temple%') LIMIT {top_k};\n",
        "\n",
        "- Question: Which hotels are located in Goa?\n",
        "- Query: SELECT name, category_level_1, category_level_2, address, region, postcode FROM read_parquet('{data_path}') WHERE (LOWER(name) LIKE '%hotel%' OR LOWER(category_level_1) LIKE '%hotel%' OR LOWER(category_level_2) LIKE '%hotel%' OR LOWER(address) LIKE '%hotel%') AND (LOWER(region) LIKE '%goa%' OR LOWER(address) LIKE '%goa%') ORDER BY CASE WHEN LOWER(name) LIKE '%hotel%' THEN 1 WHEN LOWER(category_level_2) LIKE '%hotel%' THEN 2 WHEN LOWER(category_level_1) LIKE '%hotel%' THEN 3 ELSE 4 END LIMIT {top_k};\n",
        "\n",
        "- Question: List petrol pumps in Chennai.\n",
        "- Query: SELECT name, category_level_1, category_level_2, address, region, postcode FROM read_parquet('{data_path}') WHERE (LOWER(name) LIKE '%petrol%' OR LOWER(category_level_1) LIKE '%petrol%' OR LOWER(category_level_2) LIKE '%petrol%' OR LOWER(address) LIKE '%petrol%') AND (LOWER(region) LIKE '%chennai%' OR LOWER(address) LIKE '%chennai%') ORDER BY CASE WHEN LOWER(name) LIKE '%petrol%' THEN 1 WHEN LOWER(category_level_2) LIKE '%petrol%' THEN 2 WHEN LOWER(category_level_1) LIKE '%petrol%' THEN 3 ELSE 4 END LIMIT {top_k};\n",
        "\n",
        "- Question: How many restaurants are in Bangalore?\n",
        "- Query: SELECT COUNT(*) as count FROM read_parquet('{data_path}') WHERE (LOWER(name) LIKE '%restaurant%' OR LOWER(category_level_1) LIKE '%restaurant%' OR LOWER(category_level_2) LIKE '%restaurant%' OR LOWER(address) LIKE '%restaurant%') AND (LOWER(region) LIKE '%bangalore%' OR LOWER(address) LIKE '%bangalore%') LIMIT {top_k};\n",
        "\n",
        "- Question: Find bookstores in Delhi.\n",
        "- Query: SELECT name, category_level_1, category_level_2, address, region, postcode FROM read_parquet('{data_path}') WHERE (LOWER(name) LIKE '%bookstore%' OR LOWER(category_level_1) LIKE '%bookstore%' OR LOWER(category_level_2) LIKE '%bookstore%' OR LOWER(address) LIKE '%bookstore%') AND (LOWER(region) LIKE '%delhi%' OR LOWER(address) LIKE '%delhi%') ORDER BY CASE WHEN LOWER(name) LIKE '%bookstore%' THEN 1 WHEN LOWER(category_level_2) LIKE '%bookstore%' THEN 2 WHEN LOWER(category_level_1) LIKE '%bookstore%' THEN 3 ELSE 4 END LIMIT {top_k};\n",
        "\"\"\"\n",
        "user_prompt = \"Question: {input}\"\n",
        "query_prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_message), (\"user\", user_prompt)])"
      ],
      "metadata": {
        "id": "1cGfuyhlSMDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a ChatBot Class"
      ],
      "metadata": {
        "id": "Wr5i81MJSTl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FourSquareChatBot:\n",
        "    \"\"\"NLP-to-SQL chatbot for querying DuckDB databases, optimized for Parquet files and FourSquare data.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, columns: list[str], llm, database: str = \":memory:\"):\n",
        "        \"\"\"\n",
        "        Initialize the chatbot with a DuckDB connection and schema.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the Parquet file (e.g., S3 URL or local path).\n",
        "            columns (list[str]): List of column names to include in the schema.\n",
        "            llm: Language model instance for generating SQL queries and answers.\n",
        "            database (str): DuckDB database path (default: ':memory:' for in-memory).\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.columns = columns\n",
        "        self.llm = llm\n",
        "        self.conn = self._get_duckdb_connection(database)\n",
        "        self.table_info = self._get_db_schema(limit=5)\n",
        "\n",
        "    def _get_duckdb_connection(self, database: str) -> duckdb.DuckDBPyConnection:\n",
        "        \"\"\"Create and configure a DuckDB connection.\"\"\"\n",
        "        con = duckdb.connect(database=database)\n",
        "        con.execute(\"INSTALL httpfs;\")\n",
        "        con.execute(\"LOAD httpfs;\")\n",
        "        con.execute(\"INSTALL spatial;\")\n",
        "        con.execute(\"LOAD spatial;\")\n",
        "        # Optional: Set S3 region if needed\n",
        "        # con.execute(\"SET s3_region='us-east-1';\")\n",
        "        return con\n",
        "\n",
        "    def _get_db_schema(self, limit=5):\n",
        "\n",
        "        \"\"\"Generate schema information from the Parquet file with sample values.\"\"\"\n",
        "\n",
        "        data_schema = f\"\"\"Columns:\\n\"\"\"\n",
        "\n",
        "        sql_query = f\"SELECT {\",\".join(self.columns)} FROM read_parquet('{self.data_path}') WHERE 1=1\"\n",
        "\n",
        "        for column in self.columns:\n",
        "            sql_query += f\" AND {column} IS NOT NULL\"\n",
        "\n",
        "        sql_query += f\" LIMIT {limit};\"\n",
        "        sample_result = self._execute_sql(sql_query)['result']\n",
        "\n",
        "        schema_details = self._execute_sql(f'DESCRIBE {sql_query}')['result']\n",
        "\n",
        "        for i, column in enumerate(self.columns):\n",
        "            data_type = schema_details[i][1]\n",
        "            sample_values = \",\".join([str(r[i]) for r in sample_result])\n",
        "            data_schema += f\"{i+1}. Name: {column} | Data Type: {data_type} | Sample values: {sample_values}\"\n",
        "            data_schema += \"\\n\"\n",
        "        return data_schema\n",
        "\n",
        "    def _execute_sql(self, sql_query: str) -> list:\n",
        "        \"\"\"Execute a SQL query and return results.\"\"\"\n",
        "        try:\n",
        "            result = self.conn.execute(sql_query).fetchall()\n",
        "\n",
        "            columns = [desc[0] for desc in self.conn.description]  # Grab column names\n",
        "            formatted_result = [dict(zip(columns, row)) for row in result]\n",
        "\n",
        "            return {\"result\": result, \"error\": None}\n",
        "        except Exception as e:\n",
        "            return f\"Error executing SQL: {str(e)}\"\n",
        "\n",
        "    def generate_sql_query(self, state: State) -> QueryOutput:\n",
        "        \"\"\"Generate a DuckDB SQL query from the user's question.\"\"\"\n",
        "        prompt = query_prompt_template.invoke(\n",
        "            {\n",
        "                \"top_k\": 10,\n",
        "                \"table_info\": self.table_info,\n",
        "                \"input\": state.question,\n",
        "                \"data_path\": self.data_path\n",
        "            }\n",
        "        )\n",
        "        response = self.llm.invoke(prompt)\n",
        "        cleaned_query = response.content.strip()\n",
        "\n",
        "        if cleaned_query.startswith(\"```sql\"):\n",
        "            cleaned_query = cleaned_query[6:]\n",
        "        if cleaned_query.endswith(\"```\"):\n",
        "            cleaned_query = cleaned_query[:-3]\n",
        "        # if not cleaned_query.endswith(\";\"):\n",
        "        #      cleaned_query += \";\"\n",
        "        return QueryOutput(query=cleaned_query.strip())\n",
        "\n",
        "    def generate_answer(self, state: State) -> dict:\n",
        "        \"\"\"Generate a conversational answer using query results.\"\"\"\n",
        "        # Execute the query and store JSON result\n",
        "\n",
        "        result = self._execute_sql(state.query)\n",
        "\n",
        "        if isinstance(result, str) and result.startswith(\"Error\"):\n",
        "            state.result = json.dumps({\"error\": result})\n",
        "            state.answer = f\"Sorry, I couldn't process your query due to an error: {result}\"\n",
        "        else:\n",
        "            state.result = result['result']\n",
        "\n",
        "            # Generate conversational answer\n",
        "            prompt = (\n",
        "                \"Given the following user question, corresponding SQL query, \"\n",
        "                \"and SQL result, answer the user question in a conversational manner.\\n\\n\"\n",
        "                f\"Question: {state.question}\\n\"\n",
        "                f\"SQL Query: {state.query}\\n\"\n",
        "                f\"SQL Result: {state.result}\"\n",
        "            )\n",
        "            response = self.llm.invoke(prompt)\n",
        "            state.answer = response.content\n",
        "\n",
        "        return {\"state\": state}\n",
        "\n",
        "    def process_question(self, question: str) -> dict:\n",
        "        \"\"\"Process a user question end-to-end and return the updated state.\"\"\"\n",
        "        state = State(question=question)\n",
        "        query_output = self.generate_sql_query(state)\n",
        "        state.query = query_output.query\n",
        "        result = self.generate_answer(state)\n",
        "        return result\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Clean up by closing the DuckDB connection.\"\"\"\n",
        "        if hasattr(self, \"conn\"):\n",
        "            self.conn.close()"
      ],
      "metadata": {
        "id": "yjktwyoHdzoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pipeline here would be\n",
        "\n",
        "write_query -> execute_query -> generate_answer"
      ],
      "metadata": {
        "id": "T3OV30tWS0NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "fsq_chat_bot = FourSquareChatBot(\n",
        "    data_path = \"/content/output.geoparquet\",\n",
        "    columns = ['name', 'category_level_1', 'category_level_2', 'address', 'region', 'postcode'],\n",
        "    llm = llm\n",
        ")\n",
        "\n",
        "## Process Query\n",
        "\n",
        "# query = \"Tell me the count for all the points that are related to manufacturing?\"\n",
        "query = \"List down some coffee shops in Jodhpur?\"\n",
        "print(fsq_chat_bot.process_question(question=query)['state'].answer)"
      ],
      "metadata": {
        "id": "vwHtHUG_xzFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a75f8b-0aed-4509-e1b6-b392710f2f00"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here are some coffee shops you can check out in Jodhpur:\n",
            "\n",
            "1. **Café Coffee Day**\n",
            "   - **Category:** Cafe, Coffee, and Tea House\n",
            "   - **Address:** OLD DHANIA HOUSE - JODHPUR, RAJASTHAN\n",
            "\n",
            "2. **Abar Boithak - The Coffee Shop**\n",
            "   - **Category:** Cafe, Coffee, and Tea House\n",
            "   - **Address:** 282, Jodhpur Pk, West Bengal, 700068\n",
            "\n",
            "3. **Café Coffee Day**\n",
            "   - **Category:** Cafe, Coffee, and Tea House\n",
            "   - **Address:** ANSAL PLAZA – JODHPUR, RAJASTHAN\n",
            "\n",
            "4. **Blue Mug Coffee and Thoughts**\n",
            "   - **Category:** Cafe, Coffee, and Tea House\n",
            "   - **Address:** 1/12, Jodhpur Park Road, Dhakuria, Selimpur, 1/121 Jodhpur Park, West Bengal, 700068\n",
            "\n",
            "5. **Cha Coffee Jalkhabar**\n",
            "   - **Category:** Cafe, Coffee, and Tea House\n",
            "   - **Address:** Jodhpur Pk, West Bengal, 700068\n",
            "\n",
            "6. **Blue Tokai Coffee Roasters**\n",
            "   - **Category:** Cafe, Coffee, and Tea House\n",
            "   - **Address:** 26, Jodhpur Pk, West Bengal, 700068\n",
            "\n",
            "7. **Rana Snacketeria**\n",
            "   - **Category:** Cafe, Coffee, and Tea House\n",
            "   - **Address:** 394A, Jodhpur Pk, West Bengal, 700068\n",
            "\n",
            "These spots should give you a nice variety of coffee experiences in the area! Enjoy your coffee adventure!\n"
          ]
        }
      ]
    }
  ]
}